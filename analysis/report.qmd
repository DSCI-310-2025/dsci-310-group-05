---
title: "DSCI 310 Project: Group 5"
author: "Nika Karimi Seffat, Ethan Wang, Gautam Arora and Kevin Li"
format:
  html:
    toc: true
    number-sections: true
toc: true
toc-title: Outline
toc-location: body
bibliography: references.bib
execute:
  echo: false
  warning: false
---

# **Car Evaluation Analysis**

```{r, echo=FALSE}
library(tidyverse)
```

## **Summary**

This project analyzes the Car Evaluation Dataset from @UCI1988, which includes six categorical attributes: buying price, maintenance cost, number of doors, passenger capacity, luggage space, and safety rating. The dataset was clean, with no missing values or duplicates, and required ordinal encoding to prepare it for machine learning analysis.

Exploratory Data Analysis (EDA) revealed balanced distributions across most features. However, the target variable used in model training showed a notable imbalance, with some categories being far more frequent than others. A k-Nearest Neighbors (kNN) model was trained to predict safety levels using the other attributes. The model achieved a moderate level of accuracy, outperforming random guessing but revealing limited predictive power.

These results suggest that individual features may not be sufficient on their own to determine safety levels, and more complex models or additional variables could improve performance. The project highlights the importance of thoughtful data preprocessing, including encoding and feature selection, in categorical machine learning tasks.

## **Introduction**

### **Background**:

Cars and personal transportation are an inevitable part of everyday life in the developed world. They play a crucial role in people's daily routines, enabling them to commute to work, travel, attend social gatherings, and explore new places. However, cars also pose significant safety risks. In 2023 alone, car accidents accounted for over 40 000 fatailities (@Wikipedia2019). Given these risks, it is no surprise that many individuals seek ways to make car travel safer while maintaining its convenience and necessity.

Research has shown that consumers are willing to invest in vehicle safety. For example, @Andersson2008 found that Swedish drivers were willing to pay a premium for improved car design and build quality to reduce the risk of injury in accidents. Additionally, design and material choices play a critical role in determining a vehicle's safety. @Nygren1983 found that factors such as a car’s weight, seatbelt design, and headrests significantly influenced accident survivability. More recently, @Richter2005 demonstrated that passive safety improvements—such as enhanced structural integrity and interior design modifications—have contributed to a measurable decline in injury rates from car accidents. These findings highlight the importance of identifying key factors that contribute to vehicle safety.

Given this context, our research project aims to answer the following question:

**Can we predict the estimated safety of a car using various attributes, such as its buying price, capacity (persons), and maintenance cost?**

To answer this question, we will use the Car Evaluation Dataset from UC Irvine’s Machine Learning Repository. This multivariate classification dataset contains six car design and classification variables. Key variables that will be central to our analysis include the car’s buying price, maintenance cost, seating capacity (in terms of the number of passengers it can accommodate), and the car’s evaluation level (categorized as unacceptable, acceptable, good, or very good).

## **Methods and Results**

### **Data Loading**

The dataset was retrieved from an online source and loaded into R for analysis using the read.csv function. This dataset contains categorical variables describing various attributes of cars, which will be used for classification.

```{r, echo=FALSE}
#| label: tbl-data
#| tbl-cap: "Data-Preview"
car_data <- read_csv("../data/original/car_data.csv")

# Display first few rows
head(car_data)
```

Although the dataset comes with predefined columns, it does not include column names when we read in the CSV file. We manually assign meaningful column names based on the UCI dataset documentation. @tbl-col

```{r, echo=FALSE}
#| label: tbl-col
#| tbl-cap: "Assigning the column names"

col_car_data <- read_csv("../data/clean/col_car_data.csv")
head(col_car_data)
```

```{r, echo=FALSE}
#| label: tbl-nan
#| tbl-cap: "Count of Missing and Duplicate Values"
# Print the table
nan_and_duplicates_summary <- read_csv("../data/clean/nan_and_duplicates_summary.csv")
head(nan_and_duplicates_summary)
```

Amazing! No missing values and duplicate rows. @tbl-nan

### **Data Types**

Since k-Nearest Neighbors (kNN) is a distance-based algorithm, it requires numerical input for feature comparisons. However, our dataset currently consists of categorical variables (all factors in R).

```{r, echo=FALSE}
#| label: tbl-enc
#| tbl-cap: "Car Data Encoded"

car_data_clean <- read_csv("../data/clean/car_clean.csv")

# Display first few rows
head(car_data_clean)
```

**Since we converted the text into numerical variables, we have attached the description of each numerical value per feature below for reference:**

**`buying` and `maint`** - low: 1 - med: 2 - high: 3 - vhigh: 4

**`doors` and `persons`** - The values are technically numerical, but they only fall into 3 or 4 categories and are not continuous.

**`lug_boot` and `safety`** - small/low: 1 - med: 2 - big/high: 3

**`class`** - unacc (unacceptable): 1 - acc (acceptable): 2 - good: 3 - vgood: 4

**The data has been cleaned and is ready for analysis!**

### **Summary of Data**

```{r, echo=FALSE}
#| label: tbl-sum
#| tbl-cap: "Summary Statistics"
summary(car_data_clean)
```

```{r, echo=FALSE}
#| label: tbl-cnt
#| tbl-cap: "Unique Occurrences Count"
# Count occurrences for each unique value in each column
unique_occurences <- read_csv("../output/eda_summary/unique_occurences.csv")
head(unique_occurences)
```

@tbl-cnt provides a clearer picture of the dataset's distribution across categorical variables. Most features—such as buying, maint, and doors—show relatively even distributions across their respective levels. Similarly, variables like safety, lug_boot, and persons appear fairly balanced across their categories, helping ensure the model does not become biased due to class imbalance in these predictors. However, the class variable is noticeably skewed, with one category representing the majority of cases while others appear far less frequently. Since this variable is not included as a target in the classification task, the imbalance does not affect the predictive modeling.

**Our x and y variables all have sufficient data points for analysis!**

### **EDA Analysis - Visualization**

![Distribution of Buying Price by Safety Level](../output/eda_summary_buying_safety.png){#fig-buy} 

![Distribution of Number of Persons by Safety Level](../output/eda_summary_persons_safety.png){#fig-num}

![Distribution of Maintenance Cost by Safety Level](../output/eda_summary_maint_safety.png){#fig-main}

**Histograms of Safety Across 3 Input Variables: Buying Price, Capacity (Number of Persons), and Maintenance Cost**

As we saw in the summary table, all the safety levels are evenly distributed across each of the independent variables.

**Initial Observations**

The histograms indicate that the safety levels (@fig-buy, @fig-num, and @fig-main) are evenly distributed across all three independent variables: buying price, number of persons, and maintenance cost. This suggests that the dataset is well-balanced with respect to safety levels, meaning that safety is not disproportionately concentrated in any particular category of these features. However, we do not observe significant variation is observed in the distribution of safety across these variables.

**Proceeding with Analysis**

While individual variables may not show a strong distinction in safety levels on their own, the interaction between multiple features could still be meaningful for classification. Just because safety is evenly distributed across each independent variable separately doesn't mean that a combination of factors (e.g., high buying price + low maintenance + more persons) won't reveal patterns that help predict safety levels.

### **kNN Classification Analysis**

```{r, echo=FALSE}
#| label: tbl-knn-metrics
#| tbl-cap: "kNN Model Performance Metrics"

model_metrics <- read_csv("../output/knn_model_metrics.csv")
best_k <- model_metrics$best_k
best_cv_accuracy <- model_metrics$cv_accuracy
test_accuracy <- model_metrics$test_accuracy
rand <- model_metrics$random_baseline

model_metrics
```

The final kNN model achieved `r test_accuracy`% accuracy, which is somewhat reflective of the CV accuracy of `r best_cv_accuracy`%. This is not a great accuracy, but it's better than random guessing, which would be `r rand`%.

### **Visualization of Analysis**

Since we are using multiple input variables for our kNN classification, it wouldn't be possible to graph it out 2D. We will do a **confusion matrix** as our visualization.

**Heatmap of Confusion Matrix for Visualization of Analysis**

```{r, echo=FALSE}
#| label: fig-conf
#| fig-cap: "Confusion Matrix to Evaluate Model Performance"
knitr::include_graphics("../output/knn_model_confusion_matrix.png")
```

## **Discussion**

### **What We Found**

Summary statistics based on unique value counts showed that each level of most features—such as "buying," "maint," and "doors"—appeared `r unique_occurences[3, "Count"]` times, while each level of "lug_boot", "persons", and "safety" occurred `r unique_occurences[13, "Count"]` times. However, the "class" variable showed clear imbalance: the "unacc" category appeared `r unique_occurences[4, "Count"]` times, while the least frequent category, "vgood," appeared only `r unique_occurences[7, "Count"]` times. Since "class" was not the target variable in the kNN analysis, this imbalance did not affect model training.


Exploratory Data Analysis (EDA) using histograms showed that "safety" levels (low, medium, high) were evenly distributed across "buying," "persons," and "maint," indicating no single feature strongly distinguished safety levels independently. The kNN model, trained to predict "safety" using other features, achieved a final accuracy of `r test_accuracy`% with an optimal k of `r best_k`, surpassing random guessing (`r rand`% for three classes) but suggesting limited predictive power. A confusion matrix heatmap visualized the model’s performance, highlighting prediction accuracies and errors across safety levels.

The dataset’s cleanliness and balanced feature distributions partially aligned with expectations, given its UCI origin, known for structured datasets. The absence of missing values and duplicates was anticipated, simplifying preprocessing. However, the uniform distribution of "safety" across individual features was unexpected. Real-world data might show biases—e.g., higher safety with lower buying prices or higher maintenance costs due to safety features—yet this dataset lacked such trends, possibly indicating synthetic balancing for algorithmic testing.

The kNN accuracy of `r test_accuracy`% was lower than ideal but not surprising. The even distribution of safety and lack of strong individual predictors suggested that kNN, reliant on feature proximity, might struggle. The slight accuracy increase to `r best_cv_accuracy`% from `r test_accuracy`% with tuning was expected, though the overall modest performance hinted at complex feature interactions or insufficient predictive signals, aligning with the EDA findings.

### **Potential Impact**

These results have practical and methodological implications. The preprocessing and encoding workflow offers a replicable approach for handling ordinal categorical data, applicable to fields like healthcare or marketing. The balanced predictors ensure unbiased model training, making this dataset a fair benchmark for algorithmic evaluation.

However, the kNN’s `r test_accuracy`% accuracy limits its utility for real-world safety prediction, such as in automotive design or consumer decisions, where higher reliability is critical. The uniform safety distribution suggests unmeasured variables or interactions influence safety, necessitating richer data or advanced models for practical use. This finding could prompt researchers to refine feature selection or engineering strategies when using distance-based methods, influencing future dataset analyses.

### **Future Questions**

The analysis raises several questions for further exploration each aiming to enhance its practical and analytical value:

1.  **Incorporating Additional Car Attributes**: Could adding car-specific variables—like crash test scores, manufacturing year, or brand reputation—enhance safety predictions? Expanding the dataset with real-world metrics, such as fuel efficiency, engine type (e.g., electric vs. gasoline), or historical recall data, might improve its utility for buyers or manufacturers. For instance, integrating standardized safety ratings (e.g., IIHS or NHTSA scores) could boost model accuracy beyond `r test_accuracy`, while manufacturing year might highlight safety evolution, and brand reputation could reflect quality consistency. This could transform the dataset into a robust tool for practical car evaluation.

2.  **Consumer Preferences and Safety Trade-offs**: How do car features like buying price or maintenance cost influence consumer perceptions of safety versus affordability? Survey data or X post analysis could reveal if buyers prioritize low-cost options over safety, guiding how manufacturers balance these factors in car design.

3.  **Environmental Impact on Car Safety**: Could environmental factors, such as typical driving conditions (urban vs. rural) or climate (e.g., snowy regions), affect safety ratings in this dataset? Linking car attributes to usage contexts might show if features like luggage boot size or door count adapt to safety needs in diverse settings.

4.  **Temporal Dynamics of Car Evaluation**: How would safety predictions change if the dataset tracked cars over time, such as pre- and post-safety regulation changes? A longitudinal approach could assess if older cars with fewer doors or lower safety ratings become outliers as standards evolve.

5.  **Cross-Cultural Car Evaluation**: Do safety priorities differ across regions (e.g., North America vs. Europe vs. Asia), and how might this dataset adapt? Incorporating regional safety standards or car preferences (e.g., compact cars in Europe) could test its global applicability, revealing cultural influences on car evaluation.

6.  **Feature Interactions in Car Evaluation**: Could combining car features like buying price and maintenance cost reveal stronger predictors of safety ratings? Exploring interaction terms (e.g., high buying price with low maintenance) might uncover patterns that influence car safety assessments more effectively, such as whether cost trade-offs correlate with safety compromises.

7.  **Alternative Models for Car Safety Prediction**: Would advanced models like random forests or neural networks outperform kNN by identifying critical car features—like luggage boot size or passenger capacity—for safety prediction? These could reveal non-linear relationships, such as how larger boot sizes might indicate family-oriented designs with higher safety standards.

In conclusion, this study establishes a foundation for analyzing categorical datasets with kNN, while highlighting limitations and inspiring future research into feature engineering, model selection, and practical applicability in car evaluation contexts.

## **References**
