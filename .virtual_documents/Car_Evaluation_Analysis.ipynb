











install.packages("tidyverse")
install.packages("class")
install.packages("caret")


# First, we will load the required libraries necessary to perform data wrangling, visualization, and analysis.
library(tidyverse) # Contains dplyr, ggplot2, and other libraries to perform data cleaning and visualization.
library(class) # For the kNN Classifier.
library(caret) # For train-test-split and cross-validation.


# Loading the dataset from the web
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data"
car_data <- read.csv(url, header = FALSE, stringsAsFactors = TRUE) # since all input variables are categorical, we set the data type to a factor.

# Display first few rows
head(car_data)





# Assigning the column names
colnames(car_data) <- c("buying", "maint", "doors", "persons", "lug_boot", "safety", "class")
head(car_data)





# Checking for missing values:
sum(is.na(car_data))





# Check for duplicate values:
# Remove duplicate rows (if any)
car_data <- car_data %>% distinct()
nrow(car_data)








# Every variable in this dataset is an ordinal variable - it falls under the categorical variables that have a natural relationship or hierarchy to them.
# We can use ordinal encoding to transform these factor variables into double so kNN can be used on them. Scaling is NOT needed here.

# Define encoding function for all categorical features (
encode_levels <- function(x) {
  case_when(
    x == "vhigh"  ~ 4,   
    x == "high"   ~ 3,
    x == "med"    ~ 2, 
    x == "low"    ~ 1,
    x == "big"    ~ 3,
    x == "small"  ~ 1,
    x == "more"   ~ 5,   # 'more' in persons column treated as 5
    x == "5more"  ~ 5,   # '5more' in doors column treated as 5
    x == "2"      ~ 2,
    x == "3"      ~ 3,
    x == "4"      ~ 4,
    x == "unacc"  ~ 1,
    x == "acc"    ~ 2,
    x == "good"   ~ 3,
    x == "vgood"  ~ 4,
    TRUE          ~ as.numeric(x) # Default conversion for numbers
  )
}

# Encoding safety separately to avoid duplicate "low", "med", "high"
encode_safety <- function(x) {
  case_when(
    x == "low"  ~ 1,
    x == "med"  ~ 2,
    x == "high" ~ 3,
    TRUE        ~ as.numeric(x)
  )
}

# Apply encoding to all columns, including safety separately
car_data_encoded <- car_data %>%
  mutate(across(-safety, encode_levels)) %>%
  mutate(safety = encode_safety(safety))  # Encode safety

# Display first few rows
head(car_data_encoded)











summary(car_data_encoded)


# Count occurrences for each unique value in each column
car_data_encoded %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  group_by(Variable, Value) %>%
  summarise(Count = n(), .groups = "drop") %>%
  arrange(Variable, desc(Count))








# Convert encoded variables back to their categorical labels
# Bar plot of safety distribution

car_data_labeled <- car_data_encoded %>%
  mutate(
    buying = factor(buying, levels = c(1, 2, 3, 4), labels = c("low", "med", "high", "vhigh")),
    maint = factor(maint, levels = c(1, 2, 3, 4), labels = c("low", "med", "high", "vhigh")),
    persons = factor(persons, levels = c(2, 4, 5), labels = c("2", "4", "5+")), 
    class = factor(class, levels = c(1, 2, 3, 4), labels = c("unacc", "acc", "good", "vgood")),
    safety = as.factor(safety)  # Keep safety as a factor for color coding
  )

# Define a larger theme for all plots
larger_theme <- theme(
  plot.title = element_text(size = 18, face = "bold"), # Larger title
  axis.title = element_text(size = 16),  # Larger axis labels
  axis.text = element_text(size = 14),  # Larger axis values
  legend.title = element_text(size = 14),  # Larger legend title
  legend.text = element_text(size = 12)  # Larger legend values
)

# Increase figure size, default size is not very readable
options(repr.plot.width = 10, repr.plot.height = 6)

# Plot 1: Distribution of Buying Price by Safety Level
ggplot(car_data_labeled, aes(x = buying, fill = safety)) +
  geom_bar(position = "dodge") +
  labs(title = "Distribution of Buying Price by Safety Level",
       x = "Buying Price", y = "Count", fill = "Safety Level") +
  theme_minimal() + larger_theme

# Plot 2: Distribution of Number of Persons by Safety Level
ggplot(car_data_labeled, aes(x = persons, fill = safety)) +
  geom_bar(position = "dodge") +
  labs(title = "Distribution of Number of Persons by Safety Level",
       x = "Number of Persons", y = "Count", fill = "Safety Level") +
  theme_minimal() + larger_theme

# Plot 3: Distribution of Maintenance Cost by Safety Level
ggplot(car_data_labeled, aes(x = maint, fill = safety)) +
  geom_bar(position = "dodge") +
  labs(title = "Distribution of Maintenance Cost by Safety Level",
       x = "Maintenance Cost", y = "Count", fill = "Safety Level") +
  theme_minimal() + larger_theme















set.seed(123)  # We put this to ensure reproducibility in our data, since splitting into training and test splits will be random.

# Convert categorical variables to factors
car_data_prepared <- car_data_encoded %>%
  mutate(safety = as.factor(safety))  # The target variable must be a factor for classification models in R

# Split the dataset into training (80%) and testing (20%) sets
# This ensures that the model is trained on one portion of the data and tested on unseen data for evaluation
train_index <- createDataPartition(car_data_prepared$safety, p = 0.8, list = FALSE)  # Randomly select 80% of data for training
train_data <- car_data_prepared[train_index, ]  # Training data
test_data <- car_data_prepared[-train_index, ]  # Testing data

# Separate features and target variable
train_x <- train_data %>% select(-safety)  # Predictor variables
train_y <- train_data$safety               # Target variable
test_x <- test_data %>% select(-safety)
test_y <- test_data$safety


# Define range of k values to test
# We test odd values from 1 to 21 (odd numbers help prevent ties in kNN classification)
k_values <- seq(1, 21, 2)  

# Create an empty dataframe to store cross-validation results
cv_results <- data.frame(k = integer(), accuracy = numeric())  

# Perform cross-validation for each k value in the defined range
for (k in k_values) {
  
  # Train kNN model using 5-fold cross-validation
  knn_model <- train(
    train_x, train_y, method = "knn",  # Specify kNN as the classification method
    trControl = trainControl(method = "cv", number = 5),  # 5-fold cross-validation to evaluate k
    tuneGrid = data.frame(k = k)  # Set the current k value in the tuning grid
  )
  
  # Extract the highest accuracy from the model results and store it
  cv_results <- rbind(cv_results, data.frame(k = k, accuracy = max(knn_model$results$Accuracy)))
}

# Find the best k (the one that resulted in the highest accuracy)
best_k <- cv_results$k[which.max(cv_results$accuracy)]
cat("Best k found through cross-validation:", best_k, "\n")  # Print the best k value


# Compute classification accuracy
# Accuracy is calculated as the proportion of correctly classified instances out of the total test set
accuracy <- sum(predictions == test_y) / length(test_y)  

# Print accuracy as a percentage
cat("kNN Model Accuracy:", round(accuracy * 100, 2), "%\n")  





# Train the final kNN model using the best k found from cross-validation
# The knn() function computes distances between test and training points
final_knn <- knn(train = train_x, test = test_x, cl = train_y, k = best_k)

# Evaluate Model Performance
# Accuracy is calculated as the proportion of correctly classified test instances
accuracy <- sum(final_knn == test_y) / length(test_y)  

# Print the final model's accuracy as a percentage with two decimal places
cat("Final kNN Model Accuracy:", round(accuracy * 100, 2), "%\n")  











# Generate the confusion matrix to evaluate model performance
# This compares the predicted labels (final_knn) with the actual test labels (test_y)
conf_matrix <- confusionMatrix(final_knn, test_y)

# Convert confusion matrix into a tidy format for plotting
# The confusionMatrix function stores results in a table format, so we extract and reformat it
conf_matrix_df <- as.data.frame(conf_matrix$table)

# Rename columns for clarity (matching true and predicted values)
colnames(conf_matrix_df) <- c("True_Label", "Predicted_Label", "Count")

# Plot a heatmap to visualize classification performance
ggplot(conf_matrix_df, aes(x = Predicted_Label, y = True_Label, fill = Count)) +
  geom_tile() +  # Creates a grid-based heatmap
  geom_text(aes(label = Count), color = "white", size = 6) +  # Add count values to each cell
  scale_fill_gradient(low = "lightblue", high = "blue") +  # Use a color gradient for better visualization
  labs(title = "Confusion Matrix for kNN Classification",
       x = "Predicted Label", y = "True Label", fill = "Count") +  # Customize labels
  theme_minimal()  # Apply a clean, minimal theme












