---
title: "DSCI 310 Project: Group 5"
author: "Nika Karimi Seffat, Ethan Wang, Gautam Arora and Kevin Li"
format:
  html:
    toc: true
    number-sections: true
  pdf:
    toc: true
    number-sections: true
toc: true
toc-title: Outline
toc-location: body
bibliography: references.bib
execute:
  echo: false
  warning: false
---

# **Car Evaluation Analysis**

## **Summary**

This project analyzes the **Car Evaluation Dataset** from @UCI1988, which contains `r nrow(car_data)` with **six categorical attributes: buying price**, maintenance cost, number of doors, passenger capacity, luggage space, and safety rating. The dataset was clean, with no missing values or duplicates, and required **ordinal encoding** for machine learning analysis.

Exploratory Data Analysis (EDA) revealed balanced distributions across most features. However, the "class" variable was highly imbalanced, with "unacceptable" accounting for **1,210 instances** and "very good" only **65 instances**. The k-Nearest Neighbors (kNN) model was trained to predict **safety levels** using the other features, achieving an accuracy of `r test_accuracy` with an optimal `r best_k`—an improvement over random guessing (**33.3%**) but highlighting the model's limited predictive power.

The findings indicate that individual features do not strongly determine safety levels independently, suggesting the need for more complex models or additional variables. These results have implications for both vehicle safety assessment and methodological best practices in categorical data analysis, reinforcing the importance of feature selection and encoding techniques for machine learning applications.

## **Introduction**

###**Background**:

Cars and personal transportation are an inevitable part of everyday life in the developed world. They play a crucial role in people's daily routines, enabling them to commute to work, travel, attend social gatherings, and explore new places. However, cars also pose significant safety risks. In 2023 alone, car accidents accounted for over 40,000 fatalities (@Wikipedia2019). Given these risks, it is no surprise that many individuals seek ways to make car travel safer while maintaining its convenience and necessity.

Research has shown that consumers are willing to invest in vehicle safety. For example, @Andersson2008 found that Swedish drivers were willing to pay a premium for improved car design and build quality to reduce the risk of injury in accidents. Additionally, design and material choices play a critical role in determining a vehicle's safety. @Nygren1983 found that factors such as a car’s weight, seatbelt design, and headrests significantly influenced accident survivability. More recently, @Richter2005 demonstrated that passive safety improvements—such as enhanced structural integrity and interior design modifications—have contributed to a measurable decline in injury rates from car accidents. These findings highlight the importance of identifying key factors that contribute to vehicle safety.

Given this context, our research project aims to answer the following question:

**Can we predict the estimated safety of a car using various attributes, such as its buying price, capacity (persons), and maintenance cost?**

To answer this question, we will use the Car Evaluation Dataset from UC Irvine’s Machine Learning Repository. This multivariate classification dataset contains six car design and classification variables, and includes `r nrow(car_data)` observations. Key variables that will be central to our analysis include the car’s buying price, maintenance cost, seating capacity (in terms of the number of passengers it can accommodate), and the car’s evaluation level (categorized as unacceptable, acceptable, good, or very good).

## **Methods and Results**

### **Data Loading**

The dataset was retrieved from an online source and loaded into R for analysis using the read.csv function. This dataset contains categorical variables describing various attributes of cars, which will be used for classification.

```{r, echo=FALSE, include=FALSE}
# Load dataset from pre-saved output (Ensure it's in memory)
car_data <- read.csv("data/original/car_data.csv", stringsAsFactors = TRUE)
```

Although the dataset comes with predefined columns, it does not include column names when we read in the CSV file. We manually assign meaningful column names based on the UCI dataset documentation.

```{r, echo=FALSE}
# Assigning the column names
colnames(car_data) <- c("buying", "maint", "doors", "persons", "lug_boot", "safety", "class")
head(car_data)
```

### **Data Wrangling and Cleaning**

```{r, echo=FALSE}
# Checking for missing values:
sum(is.na(car_data))
```

Amazing! No missing values to drop or account for.

```{r, echo=FALSE}
# Check for duplicate values:
# Remove duplicate rows (if any)
car_data <- car_data %>% distinct()
nrow(car_data)
```

We have the same number of rows as before - there were no duplicate rows!

### **Data Types**

Since k-Nearest Neighbors (kNN) is a distance-based algorithm, it requires numerical input for feature comparisons. However, our dataset currently consists of categorical variables (all factors in R).

```r
# Load cleaned dataset
car_clean <- read.csv("data/clean/car_clean.csv")
```

**Since we converted the text into numerical variables, we have attached the description of each numerical value per feature below for reference:**

**`buying` and `maint`** - low: 1 - med: 2 - high: 3 - vhigh: 4

**`doors` and `persons`** - The values are technically numerical, but they only fall into 3 or 4 categories and are not continuous.

**`lug_boot` and `safety`** - small/low: 1 - med: 2 - big/high: 3

**`class`** - unacc (unacceptable): 1 - acc (acceptable): 2 - good: 3 - vgood: 4

**The data has been cleaned and is ready for analysis!**

### **Summary of Data**

Table 1: Summary Statistics of Cleaned Data

```{r, echo=FALSE}
#| label: tbl-summary-stats
#| tbl-cap: "Summary Statistics of Cleaned Data."
summary_df <- readRDS("output/eda_summary_stats.rds")
kable(summary_df)
```

### **Frequency Counts of Features**

Table 2: Frequency Count of Categorical Features

```{r, echo=FALSE}
#| label: tbl-frequency-counts
#| tbl-cap: "Frequency Count of Categorical Features."
value_counts <- readRDS("output/eda_value_counts.rds")
kable(value_counts)
```

This frequency table provides a clearer picture of the dataset's distribution across categorical variables. The `buying`, `maint`, and `doors` features all have an even distribution, with each level appearing 432 times, suggesting a balanced dataset for these features. The `safety`, `lug_boot` and `persons` features are also evenly distributed across its three levels, with 576 occurrences each, ensuring no significant class imbalance in this variable. However, the `class` is highly imbalanced, with "unacc" (encoded as 1) making up the majority (1,210 occurrences), while "vgood" (encoded as 4) is rare (only 65 occurrences). Since we are not using the `class` variable in our analysis, this is not much of a concern.

**Our x and y variables all have sufficient data points for analysis!**

### **EDA Analysis - Visualization**

#### **Distribution of Buying Price by Safety Level**

Figure 1: Distribution of Buying Price by Safety Level

```{r, echo=FALSE}
#| label: fig-buying-safety
#| fig-cap: "Distribution of Buying Price by Safety Level."
knitr::include_graphics("output/eda_buying_safety.png")
```

#### **Distribution of Number of Persons by Safety Level**

Figure 2: Distribution of Number of Persons by Safety Level

```{r, echo=FALSE}
#| label: fig-persons-safety
#| fig-cap: "Distribution of Number of Persons by Safety Level."
knitr::include_graphics("output/eda_persons_safety.png")
```

#### **Distribution of Maintenance Cost by Safety Level**

Figure 3: Distribution of Maintenance Cost by Safety Level

```{r, echo=FALSE}
#| label: fig-maint-safety
#| fig-cap: "Distribution of Maintenance Cost by Safety Level."
knitr::include_graphics("output/eda_maint_safety.png")
```

As we saw in the summary table, all the safety levels are evenly distributed across each of the independent variables.

**Initial Observations**

The histograms indicate that the safety levels (1, 2, and 3) are evenly distributed across all three independent variables: buying price, number of persons, and maintenance cost. This suggests that the dataset is well-balanced with respect to safety levels, meaning that safety is not disproportionately concentrated in any particular category of these features. However, we do not observe significant variation is observed in the distribution of safety across these variables.

**Proceeding with Analysis**

While individual variables may not show a strong distinction in safety levels on their own, the interaction between multiple features could still be meaningful for classification. Just because safety is evenly distributed across each independent variable separately doesn't mean that a combination of factors (e.g., high buying price + low maintenance + more persons) won't reveal patterns that help predict safety levels.

### **kNN Classification Analysis**

```{r, echo=FALSE}
set.seed(123)  # We put this to ensure reproducibility in our data, since splitting into training and test splits will be random.

# Convert categorical variables to factors
car_data_prepared <- car_data_encoded %>%
  mutate(safety = as.factor(safety))  # The target variable must be a factor for classification models in R

# Split the dataset into training (80%) and testing (20%) sets
# This ensures that the model is trained on one portion of the data and tested on unseen data for evaluation
train_index <- createDataPartition(car_data_prepared$safety, p = 0.8, list = FALSE)  # Randomly select 80% of data for training
train_data <- car_data_prepared[train_index, ]  # Training data
test_data <- car_data_prepared[-train_index, ]  # Testing data

# Separate features and target variable
train_x <- train_data %>% select(-safety)  # Predictor variables
train_y <- train_data$safety               # Target variable
test_x <- test_data %>% select(-safety)
test_y <- test_data$safety
```

```{r, echo=FALSE}
# Define k values to test (odd values to prevent ties)
k_values <- seq(1, 21, 2)

# Store cross-validation results
cv_results <- data.frame(k = integer(), accuracy = numeric())  

# Perform cross-validation for each k value
for (k in k_values) {
  set.seed(123)  # Ensure reproducibility in CV

  knn_model <- train(
    train_x, train_y, method = "knn",
    trControl = trainControl(method = "cv", number = 5),  # 5-fold CV
    tuneGrid = data.frame(k = k)
  )

  # Store cross-validation accuracy
  cv_results <- rbind(cv_results, data.frame(k = k, accuracy = max(knn_model$results$Accuracy)))
}

# Best k based on cross-validation accuracy
best_k <- cv_results$k[which.max(cv_results$accuracy)]
best_cv_accuracy <- max(cv_results$accuracy)  # Best cross-validation accuracy

cat("Best k found through cross-validation:", best_k, "\n")
cat("Best cross-validation accuracy:", round(best_cv_accuracy * 100, 2), "%\n")
```

```{r, echo=FALSE}
# Train the final model with the best k
final_knn_model <- train(
  train_x, train_y, method = "knn",
  trControl = trainControl(method = "none"),  # No CV, just final training
  tuneGrid = data.frame(k = best_k)
)

# Generate predictions on the test set
predictions <- predict(final_knn_model, test_x)

# Compute test accuracy
test_accuracy <- sum(predictions == test_y) / length(test_y)

cat("Test Set Accuracy:", round(test_accuracy * 100, 2), "%\n")
```

The final kNN model achieved `r test_accuracy` accuracy, which is somewhat reflective of the CV accuracy of `r best_cv_accuracy`. This is not a great accuracy, but it's better than random guessing, which would be \~33.33%.

### **Visualization of Analysis**

Since we are using multiple input variables for our kNN classification, it wouldn't be possible to graph it out 2D. We will do a **confusion matrix** as our visualization.

```{r, echo=FALSE}
# Generate the confusion matrix to evaluate model performance
# This compares the predicted labels (final_knn) with the actual test labels (test_y)
conf_matrix <- confusionMatrix(predictions, test_y)

# Convert confusion matrix into a tidy format for plotting
# The confusionMatrix function stores results in a table format, so we extract and reformat it
conf_matrix_df <- as.data.frame(conf_matrix$table)

# Rename columns for clarity (matching true and predicted values)
colnames(conf_matrix_df) <- c("True_Label", "Predicted_Label", "Count")

# Plot a heatmap to visualize classification performance
ggplot(conf_matrix_df, aes(x = Predicted_Label, y = True_Label, fill = Count)) +
  geom_tile() +  # Creates a grid-based heatmap
  geom_text(aes(label = Count), color = "white", size = 6) +  # Add count values to each cell
  scale_fill_gradient(low = "lightblue", high = "blue") +  # Use a color gradient for better visualization
  labs(title = "Confusion Matrix for kNN Classification",
       x = "Predicted Label", y = "True Label", fill = "Count") +  # Customize labels
  theme_minimal()  # Apply a clean, minimal theme
```

**Figure 2: Heatmap of Confusion Matrix for Visualization of Analysis**

## **Discussion**

### **What We Found**

The analysis started with a car evaluation dataset lacking column names, which were manually assigned as "buying," "maint," "doors," "persons," "lug_boot," "safety," and "class" based on UCI documentation. The dataset, comprising `r nrow(car_data)` rows, was clean with no missing values or duplicates. All variables were categorical and ordinal, necessitating ordinal encoding into numerical values for k-Nearest Neighbors (kNN) analysis. Summary statistics revealed balanced distributions across most features (e.g., "buying," "maint," "doors," "lug_boot," "safety," and "persons"), with each category appearing 432 or 576 times. However, the "class" variable was highly imbalanced, with "unacc" dominating (1,210 instances) and "vgood" rare (65 instances), though this was irrelevant as "class" was not used in the kNN analysis.

Exploratory Data Analysis (EDA) using histograms showed that "safety" levels (low, medium, high) were evenly distributed across "buying," "persons," and "maint," indicating no single feature strongly distinguished safety levels independently. The kNN model, trained to predict "safety" using other features, achieved a final accuracy of `r test_accuracy` with an optimal k of `r best_k`, surpassing random guessing (33.3% for three classes) but suggesting limited predictive power. A confusion matrix heatmap visualized the model’s performance, highlighting prediction accuracies and errors across safety levels.

The dataset’s cleanliness and balanced feature distributions partially aligned with expectations, given its UCI origin, known for structured datasets. The absence of missing values and duplicates was anticipated, simplifying preprocessing. However, the uniform distribution of "safety" across individual features was unexpected. Real-world data might show biases—e.g., higher safety with lower buying prices or higher maintenance costs due to safety features—yet this dataset lacked such trends, possibly indicating synthetic balancing for algorithmic testing.

The kNN accuracy of `r test_accuracy` was lower than ideal but not surprising. The even distribution of safety and lack of strong individual predictors suggested that kNN, reliant on feature proximity, might struggle. The slight accuracy increase to `r best_cv_accuracy` from `r test_accuracy` with tuning was expected, though the overall modest performance hinted at complex feature interactions or insufficient predictive signals, aligning with the EDA findings.

### **Potential Impact**

These results have practical and methodological implications. The preprocessing and encoding workflow offers a replicable approach for handling ordinal categorical data, applicable to fields like healthcare or marketing. The balanced predictors ensure unbiased model training, making this dataset a fair benchmark for algorithmic evaluation.

However, the kNN’s `r test_accuracy` accuracy limits its utility for real-world safety prediction, such as in automotive design or consumer decisions, where higher reliability is critical. The uniform safety distribution suggests unmeasured variables or interactions influence safety, necessitating richer data or advanced models for practical use. This finding could prompt researchers to refine feature selection or engineering strategies when using distance-based methods, influencing future dataset analyses.

### **Future Questions**

The analysis raises several questions for further exploration each aiming to enhance its practical and analytical value:

1.  **Incorporating Additional Car Attributes**: Could adding car-specific variables—like crash test scores, manufacturing year, or brand reputation—enhance safety predictions? Expanding the dataset with real-world metrics, such as fuel efficiency, engine type (e.g., electric vs. gasoline), or historical recall data, might improve its utility for buyers or manufacturers. For instance, integrating standardized safety ratings (e.g., IIHS or NHTSA scores) could boost model accuracy beyond `r test_accuracy`, while manufacturing year might highlight safety evolution, and brand reputation could reflect quality consistency. This could transform the dataset into a robust tool for practical car evaluation.

2.  **Consumer Preferences and Safety Trade-offs**: How do car features like buying price or maintenance cost influence consumer perceptions of safety versus affordability? Survey data or X post analysis could reveal if buyers prioritize low-cost options over safety, guiding how manufacturers balance these factors in car design.

3.  **Environmental Impact on Car Safety**: Could environmental factors, such as typical driving conditions (urban vs. rural) or climate (e.g., snowy regions), affect safety ratings in this dataset? Linking car attributes to usage contexts might show if features like luggage boot size or door count adapt to safety needs in diverse settings.

4.  **Temporal Dynamics of Car Evaluation**: How would safety predictions change if the dataset tracked cars over time, such as pre- and post-safety regulation changes? A longitudinal approach could assess if older cars with fewer doors or lower safety ratings become outliers as standards evolve.

5.  **Cross-Cultural Car Evaluation**: Do safety priorities differ across regions (e.g., North America vs. Europe vs. Asia), and how might this dataset adapt? Incorporating regional safety standards or car preferences (e.g., compact cars in Europe) could test its global applicability, revealing cultural influences on car evaluation.

6.  **Feature Interactions in Car Evaluation**: Could combining car features like buying price and maintenance cost reveal stronger predictors of safety ratings? Exploring interaction terms (e.g., high buying price with low maintenance) might uncover patterns that influence car safety assessments more effectively, such as whether cost trade-offs correlate with safety compromises.

7.  **Alternative Models for Car Safety Prediction**: Would advanced models like random forests or neural networks outperform kNN by identifying critical car features—like luggage boot size or passenger capacity—for safety prediction? These could reveal non-linear relationships, such as how larger boot sizes might indicate family-oriented designs with higher safety standards.

In conclusion, this study establishes a foundation for analyzing categorical datasets with kNN, while highlighting limitations and inspiring future research into feature engineering, model selection, and practical applicability in car evaluation contexts.

## **References**